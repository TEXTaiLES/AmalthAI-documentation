{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AmalthAI Documentation Portal","text":"<p>AmalthAI is a machine learning platform developed as part of the TEXTaiLES toolbox which offers end-to-end interaction with the whole machine learning project pipeline using an intuitive interface.</p> <p>AmalthAI is designed for uses in cultural heritage while follows an easy-to-interact architecture. </p> <p> </p> <p>Technologies used in AmalthAI include:</p> <p> </p>"},{"location":"deployment/installation_advanced/","title":"Advanced Settings","text":""},{"location":"deployment/installation_advanced/#further-instructions-for-advanced-users","title":"Further Instructions for Advanced Users","text":""},{"location":"deployment/installation_advanced/#manage-your-deployments-using-kubeflow","title":"Manage your deployments using Kubeflow","text":"<p>To further view and manage your Katib experiments, you can use the Kubeflow Dashboard. The dashboard provides a user-friendly interface to monitor the status of your deployments and access various Kubeflow components. As platform administrator you can start the daskboard by running the following command:</p> <pre><code>kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80\n</code></pre> <p>Then, open your web browser and navigate to <code>http://localhost:8080/</code>. From there, you can access the Kubeflow Dashboard and explore the different components, including Katib.</p> <p> </p>"},{"location":"deployment/installation_advanced/#monitor-current-running-pods-for-debugging","title":"Monitor current running pods for debugging","text":"<p>To monitor the current running pods in your Kubernetes cluster, you can use the following command:</p> <pre><code>kubectl get pods -n name-of-the-namespace\n</code></pre> <p>There you will see a list of all the pods running in the specified namespace, along with their status, age, and other relevant information. This command is useful for debugging purposes, as it allows you to check the status of your deployments and identify any issues that may arise.</p>"},{"location":"deployment/installation_advanced/#yaml-file-structure-for-experiments","title":"YAML file structure for experiments","text":"<p>Each experiment in Katib is defined using a YAML file that specifies the configuration and parameters for the experiment. Below is an example of a YAML file structure for a simple experiment that uses the Random Search algorithm to optimize hyperparameters for a segmentation task:</p> <pre><code>apiVersion: kubeflow.org/v1beta1\nkind: Experiment\nmetadata:\n  name: random-experiment\n  namespace: kubeflow-user-example-com  # Ensure this matches your Kubeflow namespace\n  annotations:\n    katib.kubeflow.org/metrics-collector-injection: enabled  # Enable metrics collection\nspec:\n  objective:\n    type: maximize\n    goal: 0.4\n    objectiveMetricName: meaniou # choose the metric to optimize\n  algorithm:\n    algorithmName: random\n  parallelTrialCount: 1\n  maxTrialCount: 1\n  maxFailedTrialCount: 1\n  parameters: # define hyperparameters to optimize\n    - name: learning-rate\n      parameterType: double\n      feasibleSpace:\n        min: \"0.01\"\n        max: \"0.1\"\n    - name: batch-size\n      parameterType: int\n      feasibleSpace:\n        min: \"2\"\n        max: \"3\"\n    - name: epochs\n      parameterType: int\n      feasibleSpace:\n        min: \"1\"\n        max: \"2\"\n  trialTemplate:\n    primaryContainerName: training-container\n    trialParameters:\n      - name: learningRate\n        description: Learning rate for the training job\n        reference: learning-rate\n      - name: batchSize\n        description: Batch size for the training job\n        reference: batch-size\n      - name: numEpochs\n        description: Number of training epochs\n        reference: epochs\n    trialSpec:\n      apiVersion: batch/v1\n      kind: Job\n      spec:\n        template:\n          metadata:\n            annotations:\n              sidecar.istio.io/inject: \"false\"\n          spec:\n            containers:\n              - name: training-container\n                image: segmentation-utils:v1\n                imagePullPolicy: IfNotPresent\n                command: # define the command to run the training script\n                  - \"python\"\n                  - \"-u\"\n                  - \"/segmentation/train.py\"\n                  - \"--config\"\n                  - \"/segmentation/config.json\"\n                  - \"--lr\"\n                  - \"${trialParameters.learningRate}\"\n                  - \"--bs\"\n                  - \"${trialParameters.batchSize}\"\n                  - \"--epochs\"\n                  - \"${trialParameters.numEpochs}\"\n                  - --localpath\n                  - add_the_timestamp\n                  - --dataset \n                  - /segmentation/dataset_path\n                volumeMounts:\n                  - mountPath: /dev/shm\n                    name: shm\n                  - mountPath: /segmentation\n                    name: segmentation\n            volumes: #define volumes to be mounted in the container\n              - name: shm\n                emptyDir:\n                  medium: Memory\n                  sizeLimit: 32Gi\n              - name: segmentation\n                hostPath:\n                  path: /host/Segmentation\n                  type: Directory\n            restartPolicy: OnFailure\n</code></pre> <p>A custom version of this script is applied every time the Start Training button is pressed in the platform UI. You can find the relevant code used for this implementation inside the <code>/Frontend/AmalthAI_WebApp/utils/experiment_organize.py</code> file.</p>"},{"location":"deployment/installation_advanced/#cvat-integration","title":"CVAT Integration","text":"<p>Make sure that CVAT is properly setted up and running inside your system and the right port is already updated on the main <code>app.py</code> file of the platform.</p>"},{"location":"deployment/installation_basic/","title":"Installation Steps","text":""},{"location":"deployment/installation_basic/#basic-installation","title":"Basic Installation","text":"<p>The whole platform architecture is depicted in the following diagram:</p> <p> </p> <p>Minimum system requirements for a local installation are:</p> <ul> <li>CPU: at least 8 cores</li> <li>RAM: at least 16 GB</li> <li>GPU: NVIDIA GPU with at least 12 GB VRAM</li> <li>OS: Ubuntu 22.04 or newer</li> <li>Browser: Google Chrome or Microsoft Edge</li> </ul>"},{"location":"deployment/installation_basic/#before-you-start","title":"Before You Start","text":"<p>Make sure that you have cloned the following repository on your local machine:</p> <pre><code>git clone https://github.com/TEXTaiLES/AmalthAI\n</code></pre> <ul> <li> <p>The <code>AmalthAI_WebApp</code> folder contains the web app code, which is necessary for the platform's UI. </p> </li> <li> <p>The <code>Backend</code> folder contains all the necessary code for the platform's functionality which includes machine learning models, data processing scripts, and deployment configurations. Ensure that the content of the backend folder is also shared across the Kubernetes cluster and the corresponding containers.</p> </li> </ul>"},{"location":"deployment/installation_basic/#step-1-docker-installation","title":"Step 1 - Docker Installation","text":"<p>Make sure that you have a local installation of Docker. You can follow the installation process described here:</p> <p> </p>"},{"location":"deployment/installation_basic/#step-2-kubernetes-installation","title":"Step 2 - Kubernetes Installation","text":"<p>Kubernetes installation is required to orchestrate the different components of the platform. You can follow the official installation tutorial here:</p> <p> </p>"},{"location":"deployment/installation_basic/#step-3-kubeflow-installation","title":"Step 3 - Kubeflow Installation","text":"<p>Kubeflow installation is also required because experiment initialization is based on it. For installation, follow the instructions here:</p> <p> </p> <p>For local development and quick testing, the kind tool can be used.</p> <p>Ensure that the Kubeflow installation:</p> <ul> <li> <p>is accessible from the Python environment, i.e. the Kubeflow Python SDK can connect to the API</p> </li> <li> <p>has at least one directory mounted as a <code>PersistentVolume</code> or a <code>hostPath</code></p> </li> <li> <p>has GPU access</p> </li> </ul> <p>When the above directory is mounted, make sure that you move every folder from the <code>Backend</code> folder inside that directory so that the three tasks are accessible from the Kubeflow pipelines.</p> <p>Important Note: Make sure that inside this folder you have created a folder named <code>Datasets</code> where all the datasets will be stored. Inside the <code>Datasets</code> folder, create three subfolders named <code>Segmentation</code>, <code>Classification</code> and <code>Object-Detection</code> for each task respectively.</p>"},{"location":"deployment/installation_basic/#step-4-docker-images-setup","title":"Step 4 - Docker Images Setup","text":"<p>Machine learning models require appropriate environments to run on. Because the platform is Kubernetes-based, there is need for ready-to-use docker containers. </p> <p>1) For the semantic segmentation and the classification mode, to run the models, a Docker container based on PyTorch is needed.</p> <p>First of all, download the basic image using the followning command:</p> <pre><code>docker pull nvcr.io/nvidia/pytorch:22.12-py3\n</code></pre> <p>After pulling the base image, you have to create an updated image with all the necessary libraries installed. To do that, change your directory to <code>Backend/Segmentation/</code> and utilize the Dockerfile inside that folder by running the following command:</p> <pre><code>docker build -t segm_cls_image .\n</code></pre> <p>The Platform's backend dynamically creates containers on demand for each segmentation, classification and object detection task. These containers are single-use and they are instantiated only for the duration of the task execution and automatically destroyed upon completion.</p> <p>2) For object detection task, you can use the official Ultralytics Docker image that has all the necessary libraries installed for running YOLO models.</p> <p>To build this image locally, you can perform the following steps:</p> <pre><code>docker pull ultralytics/ultralytics:latest-python-export\n</code></pre> <p> </p> <p>Important Note: Make sure that you keep the config.yml file updated inside <code>/AmalthAI_WebApp</code> folder with the correct image names and the shared directory path where the <code>Segmentation</code>, <code>Classification</code> and <code>ObjectDetection</code> folders are located.</p>"},{"location":"deployment/installation_basic/#step-5-upload-docker-images-into-kind-cluster","title":"Step 5 - Upload docker images into kind cluster","text":"<p>To upload a locally built Docker image to your Kubernetes cluster, you have to run the following command:</p> <pre><code>kind load docker-image myimage:latest --name name-of-your-cluster\n</code></pre> <p>For the AmalthAI platform, you have to upload both the <code>segm_cls_image</code> and the <code>ultralytics/ultralytics:latest-python-export</code> images into the kind cluster.</p>"},{"location":"deployment/installation_basic/#step-6-cvat-installation","title":"Step 6 - CVAT Installation","text":"<p>For the annotation purposes of this platform, we utilize CVAT annotation tool. To install it on your system, follow the instructions that are provided here.</p> <p> </p>"},{"location":"deployment/installation_basic/#step-7-platform-ui-setup","title":"Step 7 - Platform UI Setup","text":"<p>For the platform's UI, you have to create a Docker container with the appropriate libraries to run the web app.</p> <p>1) First open your terminal inside the <code>/AmalthAI_WebApp</code> folder and run:</p> <pre><code>docker build -t amalthai .\n</code></pre> <p>2) After the build is completed, you can run the container with the following command:</p> <pre><code>docker run --rm --network=host -v $(pwd):/app -v /var/run/docker.sock:/var/run/docker.sock -v /path/to/backend/shared/folder:/data -v ~/.kube/config:/root/.kube/config amalthai\n</code></pre> <p>Then you can easily navigate to the web app, by opening your browser and going to:</p> <pre><code>http://0.0.0.0:5000\n</code></pre>"},{"location":"expansions/expansions/","title":"Platform Expansions","text":"<p>This page is dedicated to documenting the various extensions and plugins available for the TEXTaiLES platform. These extensions enhance the core functionality of TEXTaiLES, allowing developers to customize their experience and add new features as needed.</p>"},{"location":"expansions/expansions/#integrate-more-machine-learning-models-for-each-task","title":"Integrate more Machine Learning models for each task","text":"<p>Developers can integrate additional machine learning models for the pre existing tasks following the guidelines provided in the following sections.</p>"},{"location":"expansions/expansions/#adding-a-new-model-for-semantic-segmentation","title":"Adding a new model for Semantic Segmentation","text":"<ol> <li>To register a new model for Semantic Segmentation, create a new Python file inside the <code>/Backend/Segmentation/models</code> and update the <code>__init__.py</code> file to include your new model.</li> <li>Update the <code>models_available_segmentation.csv</code> on the <code>/AmalthAI_WebApp/data/</code> folder.</li> <li>Create a new json following the existing structure in <code>/Backend/Segmentation/configfiles</code> and name it using the model name.</li> <li>Update the training options inside the <code>training_button_segmentation.py</code> by adding your model to the list of the available models.</li> </ol> <p>Its important to note that the first column of the <code>models_available_segmentation.csv</code> file should match the name of the created json file in step 3. In the second column, you can write the display name of the model that will appear in the platform UI.</p>"},{"location":"expansions/expansions/#adding-a-new-model-for-object-detection","title":"Adding a new model for Object Detection","text":"<p>Currently, AmalthAI is designed to use only YOLO family models using the Ultralytics package. Furthermore, the annotation settings are properly configured to work with YOLO models.</p> <p>If you want to add a new model from the YOLO family, you can do so by following these steps:</p> <ol> <li>Create a new Python file for your model inside the <code>/Backend/ObjectDetection/ultralytics</code> directory named <code>yolovX.py</code> where X is the version. This will be used to train the model. Follow the other pre-existing files as examples.</li> <li>Update the <code>models_available_object_detection.csv</code> on the <code>/AmalthAI_WebApp/data/</code> folder to include your new model.</li> <li>Update the training options inside the <code>training_button_object_detection.py</code> by adding your model to the list of the available models.</li> </ol>"},{"location":"expansions/expansions/#adding-a-new-model-for-image-classification","title":"Adding a new model for Image Classification","text":"<ol> <li>To register a new model for Image Classification, create a new Python file inside the <code>/Backend/Classification/models</code> and update the <code>model_factory.py</code> file to include your new model. Follow the other pre-existing registrations as examples.</li> <li>Update the <code>models_available_classification.csv</code> on the <code>/AmalthAI_WebApp/data/</code> folder to include your new model.</li> <li>Update the training options inside the <code>training_button_classification.py</code> by adding your model to the list of the available models.</li> </ol> <p>To ensure proper integration of your new model, make sure it arrives from the Torchvision models. You can refer to the Torchvision Models Documentation for more details on available models and their usage.</p>"},{"location":"info/general_info/","title":"Project License","text":"<p>This project is licensed under the AGPL-3.0 License. For more details, please refer to the LICENSE. </p>"},{"location":"info/general_info/#citation","title":"Citation","text":"<p>If you use this software, please cite it using the following BibTeX entry:</p> <pre><code>@software{Athena_Research_Center_AmalthAI_Machine_Learning_2025,\nauthor = {{Athena Research Center}},\nlicense = {AGPL-3.0},\nmonth = nov,\ntitle = {{AmalthAI Machine Learning Platform}},\nurl = {https://github.com/TEXTaiLES/AmalthAI},\nversion = {0.1.0},\nyear = {2025}\n}\n</code></pre>"},{"location":"manual/classification/","title":"Classification Mode","text":"<p>In classification mode, the model assigns a single label to an entire image, indicating the primary object or scene depicted. This allows the user to train the model to recognize and categorize images based on their content.</p>"},{"location":"manual/classification/#dataset-creation","title":"Dataset Creation","text":"<p>To start creating a dataset for classification, you need to organize your images into folders, where each folder represents a different class. For example, if you are classifying images with type of materials, you need to have separate folders for \"wool\", \"cotton\" and \"silk\". Once your images are organized, you can upload them to the platform and create a new classification task. The structure should look like this:</p> <pre><code>DatasetName/\n\u2502\n\u251c\u2500\u2500 Class1/\n\u2502   \u251c\u2500\u2500 obj1.jpg\n\u2502   \u251c\u2500\u2500 obj2.jpg\n\u251c\u2500\u2500 Class2/\n\u2502   \u251c\u2500\u2500 obj3.jpg\n\u2502   \u251c\u2500\u2500 obj4.jpg\n\u251c\u2500\u2500 Class3/\n\u2502   \u251c\u2500\u2500 obj5.jpg\n\u2502   \u251c\u2500\u2500 obj6.jpg\n</code></pre>"},{"location":"manual/classification/#annotation-process","title":"Annotation Process","text":"<p>Once the dataset is created based on the previous mentioned structure, you can compress it as .zip file and upload it to the platform. The platform will automatically recognize the folder structure and register the dataset accordingly.</p> <p>When you complete the upload, the platform will display the dataset existance in the Collections page, showing the number of images it contains.</p>"},{"location":"manual/classification/#training-initiation","title":"Training Initiation","text":"<p>After completing the Dataset upload, you can initiate the training of your classification model. Follow these steps:</p> <ol> <li>Navigate to the Train an ML model page.</li> <li>Select the Classification mode.</li> </ol> <p>There are two main fields that need to be filled:</p> <ul> <li>Select Model: Choose the model architecture you want to use for training.</li> <li>Select Collection: Provide a name for your model.</li> </ul> <p>For the available models, you can find information about them by clicking on the Currently Available models arrow and click on the preferred model to see more details about its architecture.</p>"},{"location":"manual/classification/#inference-process","title":"Inference Process","text":"<p>Once the model is trained, you can use it for inference on new images. To perform inference, follow these steps:</p> <ol> <li>Navigate to the Inference page.</li> <li>Select the trained semantic segmentation model from the list.</li> <li>Upload the image you want to perform inference on.</li> <li>Click the Run Inference button to see the segmentation results.</li> </ol> <p>Once the Inference process is complete, the result will be displayed directly into the same page, showing the origin image along with the corresponding class and confidence score.</p> <p> </p>"},{"location":"manual/object_detection/","title":"Object Detection Mode","text":"<p>In object detection mode, the model identifies and locates objects within an image by drawing bounding boxes around them. This allows the user to train the model to recognize and classify multiple objects in a single image.</p>"},{"location":"manual/object_detection/#dataset-creation","title":"Dataset Creation","text":"<p>To start creating a dataset for Object Detection, follow these steps:</p> <ol> <li>Click the View Collections button on the Home page.</li> <li>Click the Add a new Detection dataset button to create a new dataset.</li> <li>Fill the appropriate information for your dataset, such as name, description and register your classes.</li> <li>Upload images to the dataset by clicking on the Upload Images region.</li> <li>Press Submit and Open Button to proceed with dataset annotation.</li> </ol> <p>The dataset creation page will look like the following:</p> <p> </p>"},{"location":"manual/object_detection/#annotation-process","title":"Annotation Process","text":"<p>Once the dataset is created, you can start annotating the images. The annotation interface provides various tools to facilitate the annotation process:</p> <ul> <li>Draw bounding box Tool: Allows users to draw bounding boxes around objects of interest to assign them to a specific class.</li> </ul> <p> </p> <p>Important Note: For Object Detection, the Draw bounding box Tool is the primary tool used for annotation.</p> <p>Once you navigate through all images and complete the annotations, click the Menu button on the top left corner and change the job state to completed. Once this process is done, you will be able to see the dataset ready for training in the Collections page for the corresponding task type.</p>"},{"location":"manual/object_detection/#training-initiation","title":"Training Initiation","text":"<p>After completing the annotation process, you can initiate the training of your semantic segmentation model. Follow these steps:</p> <ol> <li>Navigate to the Train an ML model page.</li> <li>Select the Object Detection mode.</li> </ol> <p>There are two main fields that need to be filled:</p> <ul> <li>Select Model: Choose the model architecture you want to use for training.</li> <li>Select Collection: Provide a name for your model.</li> </ul> <p>For the available models, you can find information about them by clicking on the Currently Available models arrow and click on the preferred model to see more details about its architecture.</p>"},{"location":"manual/object_detection/#inference-process","title":"Inference Process","text":"<p>Once the model is trained, you can use it for inference on new images. To perform inference, follow these steps:</p> <ol> <li>Navigate to the Inference page.</li> <li>Select the trained Object Detection model from the list.</li> <li>Upload the image you want to perform inference on.</li> <li>Click the Run Inference button to see the detection results.</li> </ol> <p>Once the Inference process is complete, the detected objects will be displayed directly into the same page, showing the bounding boxes around the recognized objects.</p> <p> </p>"},{"location":"manual/semantic_segm/","title":"Semantic Segmentation Mode","text":"<p>In semantic segmentation mode, the model classifies each pixel in an image into a predefined set of classes. By defining a set of classes during the dataset annotation process, the user can train the model to recognize and segment different regions of interest within an image.</p>"},{"location":"manual/semantic_segm/#dataset-creation","title":"Dataset Creation","text":"<p>To start creating a dataset for semantic segmentation, follow these steps:</p> <ol> <li>Click the View Collections button on the Home page.</li> <li>Click the Add a new Segmentation dataset button to create a new dataset.</li> <li>Fill the appropriate information for your dataset, such as name, description and register your classes.</li> <li>Upload images to the dataset by clicking on the Upload Images region.</li> <li>Press Submit and Open Button to proceed with dataset annotation.</li> </ol> <p>The dataset creation page will look like the following:</p> <p> </p>"},{"location":"manual/semantic_segm/#annotation-process","title":"Annotation Process","text":"<p>Once the dataset is created, you can start annotating the images. The annotation interface provides various tools to facilitate the annotation process:</p> <ul> <li>Draw mask Tool: Allows users to paint over regions of interest using a brush to assign them to a specific class.</li> <li>Eraser Tool: Enables users to remove parts of the mask that were incorrectly annotated.</li> <li>Polygon/Rectangle Selection Tool: Enables users to create precise masks by outlining the area of interest with polygons or rectangles.</li> </ul> <p> </p> <p>Important Note: For Semantic Segmentation, the Draw mask Tool is primarily used especially when dealing with complex shapes and regions.</p> <p>Once you navigate through all images and complete the annotations, click the Menu button on the top left corner and change the job state to completed. Once this process is done, you will be able to see the dataset ready for training in the Collections page for the corresponding task type.</p>"},{"location":"manual/semantic_segm/#training-initiation","title":"Training Initiation","text":"<p>After completing the annotation process, you can initiate the training of your semantic segmentation model. Follow these steps:</p> <ol> <li>Navigate to the Train an ML model page.</li> <li>Select the Semantic Segmentation mode.</li> </ol> <p>There are two main fields that need to be filled:</p> <ul> <li>Select Model: Choose the model architecture you want to use for training.</li> <li>Select Collection: Provide a name for your model.</li> </ul> <p>For the available models, you can find information about them by clicking on the Currently Available models arrow and click on the preferred model to see more details about its architecture.</p>"},{"location":"manual/semantic_segm/#inference-process","title":"Inference Process","text":"<p>Once the model is trained, you can use it for inference on new images. To perform inference, follow these steps:</p> <ol> <li>Navigate to the Inference page.</li> <li>Select the trained semantic segmentation model from the list.</li> <li>Upload the image you want to perform inference on.</li> <li>Click the Run Inference button to see the segmentation results.</li> </ol> <p>Once the Inference process is complete, the segmented image will be displayed directly into the same page, showing the different regions classified according to the predefined classes.</p> <p> </p>"},{"location":"manual/ui/","title":"User Interface","text":"<p>The main modalities of the AmalthAI user interface are described below.</p> <p> </p>"},{"location":"manual/ui/#1-dataset-creation-and-annotation","title":"1. Dataset Creation and Annotation","text":"<p>As the full machine learning pipeline requires annotated data, AmalthAI provides an intuitive interface for creating and previewing datasets. Also, using the integrated CVAT Annotation tool, users can annotate images and save them directly into the project structure.</p> <p> </p>"},{"location":"manual/ui/#2-model-training","title":"2. Model Training","text":"<p>AmalthAI allows users to train machine learning models directly from the user interface. More specifically, it allows users to select datasets and models through a few number of clicks. For the training parameters, users have to choose between easy and advanced mode. In easy mode, users can start training with default (preselected) parameters, while advanced mode enables manual configuration of training parameters. The predefined training parameters are based on best practices and research in the field of computer vision.</p> <p> </p>"},{"location":"manual/ui/#3-model-evaluation","title":"3. Model Evaluation","text":"<p>After the successful training on the provided dataset, the best model among the different trials and epochs is selected based on the evaluation metrics. The evaluation metrics are displayed in the user interface, allowing users to assess the model's performance.</p> <p> </p>"},{"location":"manual/ui/#4-model-inference","title":"4. Model Inference","text":"<p>Once a model is trained and evaluated, users can perform inference on new images directly from the user interface. The results of the inference are displayed directly into the page allowing users to visualize the predictions made by the model.</p> <p> </p>"}]}